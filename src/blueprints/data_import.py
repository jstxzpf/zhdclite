"""
æ•°æ®å¯¼å…¥æ¨¡å—è“å›¾
åŒ…å«å·²ç¼–ç æ•°æ®ã€åœ°æ–¹ç‚¹æ•°æ®ã€å›½å®¶ç‚¹æ•°æ®çš„å¯¼å…¥åŠŸèƒ½
"""

from flask import Blueprint, request
from werkzeug.utils import secure_filename
import os
import logging
import uuid
import pandas as pd
import re

# åˆ›å»ºè“å›¾
data_import_bp = Blueprint('data_import', __name__)
logger = logging.getLogger(__name__)

# è¿™äº›å˜é‡å°†åœ¨è“å›¾æ³¨å†Œæ—¶ä»ä¸»åº”ç”¨ä¼ å…¥
db = None
excel_ops = None
handle_errors = None
allowed_file = None
validate_file_size = None
app_config = None

def init_blueprint(database, excel_operations, error_handler, file_validator, size_validator, config):
    """åˆå§‹åŒ–è“å›¾ä¾èµ–"""
    global db, excel_ops, handle_errors, allowed_file, validate_file_size, app_config
    db = database
    excel_ops = excel_operations
    handle_errors = error_handler
    allowed_file = file_validator
    validate_file_size = size_validator
    app_config = config

def _cleanup_file(file_path):
    """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
    try:
        if file_path and os.path.exists(file_path):
            os.remove(file_path)
            logger.info(f"ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†: {file_path}")
    except Exception as e:
        logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")

def _read_and_process_csv(file_path):
    """å°è£…äº†è¯»å–å’Œé¢„å¤„ç†å›½å®¶ç‚¹CSVæ–‡ä»¶çš„é€»è¾‘"""
    # å°è¯•ä½¿ç”¨ä¸åŒçš„ç¼–ç è¯»å–CSV
    encodings_to_try = ['utf-8', 'gbk', 'gb2312']
    df = None
    for encoding in encodings_to_try:
        try:
            # è·³è¿‡ç¬¬ä¸€è¡ŒæŸåçš„è¡¨å¤´ï¼Œå¹¶ä¸”ä¸å°†æ•°æ®çš„ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
            df = pd.read_csv(file_path, encoding=encoding, header=None, skiprows=1, dtype=str)
            logger.info(f"æˆåŠŸä½¿ç”¨ {encoding} ç¼–ç è¯»å–CSVæ–‡ä»¶ï¼Œå·²è·³è¿‡é¦–è¡Œ")
            break
        except UnicodeDecodeError:
            continue
    
    if df is None:
         raise ValueError("æ— æ³•ä½¿ç”¨å¸¸ç”¨ç¼–ç  (UTF-8, GBK) è¯»å–CSVæ–‡ä»¶")

    if df.empty:
        logger.warning("CSVæ–‡ä»¶ä¸ºç©º")
        raise ValueError("CSVæ–‡ä»¶ä¸ºç©º")
    
    # ä¿®æ­£ç»Ÿè®¡å±€CSVæ–‡ä»¶çš„å­—æ®µ
    df = _fix_statistical_csv_columns(df)
    logger.info("CSVæ–‡ä»¶å­—æ®µä¿®æ­£å®Œæˆ")
    return df

def _process_uploaded_file(file, operation_name, allowed_extensions, read_func):
    """é€šç”¨çš„æ–‡ä»¶ä¸Šä¼ ã€éªŒè¯ã€ä¿å­˜å’Œè¯»å–é€»è¾‘"""
    # 1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not file or file.filename == '':
        logger.warning(f"{operation_name}æ—¶æ–‡ä»¶åä¸ºç©º")
        return None, ("æœªé€‰æ‹©æ–‡ä»¶", 400), None

    # 2. æ–‡ä»¶æ‰©å±•åå®‰å…¨éªŒè¯
    file_ext = file.filename.rsplit('.', 1)[1].lower()
    if '.' not in file.filename or file_ext not in allowed_extensions:
        logger.warning(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file.filename}")
        return None, (f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œä»…æ”¯æŒ {', '.join(allowed_extensions)} æ ¼å¼", 400), None

    # 3. æ–‡ä»¶å¤§å°éªŒè¯
    if not validate_file_size(file):
        logger.warning(f"æ–‡ä»¶å¤§å°è¶…è¿‡é™åˆ¶: {file.filename}")
        return None, ("æ–‡ä»¶å¤§å°è¶…è¿‡50MBé™åˆ¶", 400), None
    
    # 4. å®‰å…¨å¤„ç†æ–‡ä»¶åå¹¶ç¡®ä¿å”¯ä¸€æ€§
    filename = f"{uuid.uuid4().hex}_{secure_filename(file.filename)}"
    file_path = os.path.join(app_config['UPLOAD_FOLDER'], filename)
    
    try:
        # 5. ä¿å­˜æ–‡ä»¶
        file.save(file_path)
        logger.info(f"æ–‡ä»¶ä¿å­˜æˆåŠŸ: {file_path}")
        
        # 6. ä½¿ç”¨å›è°ƒå‡½æ•°è¯»å–å’Œå¤„ç†æ–‡ä»¶å†…å®¹
        df = read_func(file_path)
        return df, None, file_path

    except Exception as e:
        # ç»Ÿä¸€å¤„ç†ä¿å­˜æˆ–è¯»å–è¿‡ç¨‹ä¸­çš„æ‰€æœ‰å¼‚å¸¸
        logger.error(f"{operation_name} å¤±è´¥: {str(e)}")
        # æ¸…ç†å¯èƒ½å·²åˆ›å»ºçš„æ–‡ä»¶
        _cleanup_file(file_path)
        return None, (f"å¤„ç†æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}", 500), None

def _fix_statistical_csv_columns(df):
    """
    ä¿®æ­£ç»Ÿè®¡å±€CSVæ–‡ä»¶çš„å­—æ®µé—®é¢˜ï¼š
    1. æ ¹æ®æ–°çš„23åˆ—å­—æ®µç»“æ„åˆ†é…æ­£ç¡®å­—æ®µå
    2. æ¸…ç†æ— æ•ˆæ•°å€¼
    3. å¤„ç†æ—¶é—´æˆ³å­—æ®µ
    """
    logger.info(f"å¾…ä¿®æ­£çš„CSVæ–‡ä»¶åŸå§‹åˆ—æ•°: {len(df.columns)}")
    
    # æ–°çš„ç»Ÿè®¡å±€CSVæ–‡ä»¶æ ‡å‡†å­—æ®µåï¼ˆ22ä¸ªå­—æ®µï¼‰
    new_standard_columns_22 = [
        'SID', 'å¿ç ', 'æ ·æœ¬ç¼–ç ', 'å¹´', 'æœˆ', 'é¡µç ', 'è¡Œç ', 'ç¼–ç ', 'æ•°é‡', 'é‡‘é¢', 
        'æ•°é‡2', 'äººç ', 'æ˜¯å¦ç½‘è´­', 'è®°è´¦æ–¹å¼', 'å“å', 'é—®é¢˜ç±»å‹', 'è®°è´¦è¯´æ˜', 
        'è®°è´¦å®¡æ ¸è¯´æ˜', 'è®°è´¦æ—¥æœŸ', 'åˆ›å»ºæ—¶é—´', 'æ›´æ–°æ—¶é—´', 'è´¦é¡µç”Ÿæˆè®¾å¤‡æ ‡è¯†'
    ]
    
    # æ ¹æ®åˆ—æ•°è¿›è¡Œå¤„ç†
    if len(df.columns) == 22:
        df.columns = new_standard_columns_22
        logger.info("æ£€æµ‹åˆ°22åˆ—æ•°æ®ï¼ŒæˆåŠŸåˆ†é…å­—æ®µå")
    elif len(df.columns) == 21:
        # å¦‚æœæ˜¯21åˆ—ï¼Œå¯èƒ½ç¼ºå°‘æœ€åä¸€åˆ—
        df.columns = new_standard_columns_22[:-1]  # ä½¿ç”¨å‰21ä¸ªå­—æ®µå
        # æ·»åŠ ç¼ºå¤±çš„æœ€åä¸€åˆ—
        df['è´¦é¡µç”Ÿæˆè®¾å¤‡æ ‡è¯†'] = ''
        logger.info("æ£€æµ‹åˆ°21åˆ—æ•°æ®ï¼ŒæˆåŠŸåˆ†é…å­—æ®µåå¹¶æ·»åŠ ç¼ºå¤±å­—æ®µ")
    else:
        logger.warning(f"åˆ—æ•°ä¸åŒ¹é…: æœŸæœ›22åˆ—ï¼Œå®é™…{len(df.columns)}åˆ—ã€‚å°†å°è¯•æŒ‰å‰22åˆ—å¤„ç†ã€‚")
        if len(df.columns) > 22:
            df = df.iloc[:, :22]
            df.columns = new_standard_columns_22
        elif len(df.columns) < 22:
            # å¦‚æœåˆ—æ•°ä¸è¶³ï¼Œä½¿ç”¨ç°æœ‰åˆ—æ•°å¯¹åº”çš„å­—æ®µå
            df.columns = new_standard_columns_22[:len(df.columns)]
            # ä¸ºç¼ºå¤±çš„åˆ—æ·»åŠ ç©ºå€¼
            for i in range(len(df.columns), 22):
                df[new_standard_columns_22[i]] = ''
        logger.info(f"å·²è°ƒæ•´åˆ—æ•°å¹¶åˆ†é…å­—æ®µå")

    # åˆ é™¤å®Œå…¨ç©ºç™½çš„è¡Œ
    df = df.dropna(how='all')
    
    # æ¸…ç†æ— æ•ˆçš„æ•°å€¼æ•°æ®ï¼Œå°† 'n.n' ç­‰éæ•°å­—å€¼è½¬æ¢ä¸ºç©ºå€¼ (NaN)
    numeric_columns = ['æ•°é‡', 'é‡‘é¢', 'æ•°é‡2']
    for col in numeric_columns:
        if col in df.columns:
            # ä½¿ç”¨ to_numeric å°†æ‰€æœ‰éæ•°å­—å€¼ï¼ˆåŒ…æ‹¬ 'n.n'ï¼‰å¼ºåˆ¶è½¬æ¢æˆ NaN
            df[col] = pd.to_numeric(df[col], errors='coerce')
    logger.info("å·²å°†æ•°å€¼åˆ—ä¸­çš„ 'n.n' ç­‰æ— æ•ˆå€¼è½¬æ¢ä¸ºç©ºå€¼")

    # è§„èŒƒåŒ–â€œç¼–ç â€åˆ—ä¸º6ä½çº¯æ•°å­—å­—ç¬¦ä¸²ï¼ˆä¿®å¤å¦‚ 311018.0 -> 311018ï¼‰
    if 'ç¼–ç ' in df.columns:
        def _normalize_code(val):
            if pd.isna(val):
                return ''
            s = str(val).strip()
            # æå–æ‰€æœ‰æ•°å­—
            digits = ''.join(re.findall(r'\d', s))
            if not digits:
                return ''
            # æˆªæ–­æˆ–è¡¥é½ä¸º6ä½
            if len(digits) >= 6:
                digits = digits[:6]
            else:
                digits = digits.zfill(6)
            return digits
        df['ç¼–ç '] = df['ç¼–ç '].apply(_normalize_code)
        logger.info("å·²è§„èŒƒåŒ–â€˜ç¼–ç â€™åˆ—ä¸º6ä½çº¯æ•°å­—å­—ç¬¦ä¸²")

    # å¤„ç†æ—¥æœŸæ—¶é—´å­—æ®µ
    if 'è®°è´¦æ—¥æœŸ' in df.columns:
        # å¦‚æœåˆ›å»ºæ—¶é—´ä¸ºç©ºï¼Œä½¿ç”¨è®°è´¦æ—¥æœŸ
        if 'åˆ›å»ºæ—¶é—´' in df.columns:
            df['åˆ›å»ºæ—¶é—´'] = df['åˆ›å»ºæ—¶é—´'].fillna(df['è®°è´¦æ—¥æœŸ'])
        else:
            df['åˆ›å»ºæ—¶é—´'] = df['è®°è´¦æ—¥æœŸ']
        logger.info("å·²å¤„ç†æ—¥æœŸæ—¶é—´å­—æ®µ")
    
    # å¤„ç†å­—æ®µåæ˜ å°„ï¼šäººç  -> äººä»£ç ï¼ˆä¸ºäº†å…¼å®¹åç»­å¤„ç†ï¼‰
    if 'äººç ' in df.columns:
        df['äººä»£ç '] = df['äººç ']
        logger.info("å·²æ·»åŠ äººä»£ç å­—æ®µæ˜ å°„")
    
    logger.info(f"ä¿®æ­£åçš„CSVæ•°æ®å½¢çŠ¶: {df.shape}")
    logger.info(f"ä¿®æ­£åçš„åˆ—å: {list(df.columns)}")
    
    return df

def _normalize_local_data_columns(df):
    """æ ‡å‡†åŒ–åœ°æ–¹ç‚¹æ•°æ®çš„åˆ—å"""
    # å®šä¹‰åˆ—åæ˜ å°„å…³ç³»
    column_mapping = {
        # æ ‡å‡†åˆ—å
        'bianma': 'bianma', 'shuliang': 'shuliang', 'jine': 'jine', 'hudaima': 'hudaima',
        'nian': 'nian', 'yue': 'yue', 'ri': 'ri',
        # å¯èƒ½çš„å˜ä½“åˆ—å
        'ç¼–ç ': 'bianma', 'ç¼–å·': 'bianma', 'ä»£ç ': 'bianma', 'æ•°é‡': 'shuliang', 'é‡‘é¢': 'jine',
        'æˆ·ä»£ç ': 'hudaima', 'æˆ·ä»£': 'hudaima', 'å¹´': 'nian', 'å¹´ä»½': 'nian', 'æœˆ': 'yue',
        'æœˆä»½': 'yue', 'æ—¥': 'ri', 'æ—¥æœŸ': 'ri', 'å¤©': 'ri',
        # è‹±æ–‡å˜ä½“
        'code': 'bianma', 'amount': 'shuliang', 'money': 'jine', 'household': 'hudaima',
        'year': 'nian', 'month': 'yue', 'day': 'ri',
    }
    df_normalized = df.copy()
    original_columns = list(df.columns)
    logger.info(f"åŸå§‹Excelåˆ—å: {original_columns}")
    new_columns = {col: column_mapping.get(str(col).strip().lower(), col) for col in df.columns}
    df_normalized.rename(columns=new_columns, inplace=True)
    required_columns = ['bianma', 'shuliang', 'jine', 'hudaima', 'nian', 'yue']
    missing_columns = [col for col in required_columns if col not in df_normalized.columns]
    if missing_columns:
        logger.error(f"ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
        raise ValueError(f"Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {missing_columns}")
    # å¤„ç†riåˆ—
    if 'ri' not in df_normalized.columns:
        logger.info("Excelæ–‡ä»¶ä¸­æ²¡æœ‰'ri'(æ—¥æœŸ)åˆ—ï¼Œå°†ä½¿ç”¨é»˜è®¤å€¼1")
        df_normalized['ri'] = '1'
    else:
        # æ¸…ç†riåˆ—ä¸­çš„æ— æ•ˆå€¼
        df_normalized['ri'] = df_normalized['ri'].astype(str)
        df_normalized['ri'] = df_normalized['ri'].replace(['nan', 'None', ''], '1')
        logger.info(f"riåˆ—æ•°æ®æ¸…ç†å®Œæˆï¼Œç©ºå€¼å·²æ›¿æ¢ä¸ºé»˜è®¤å€¼1")

    logger.info(f"åˆ—åæ ‡å‡†åŒ–å®Œæˆ: {original_columns} -> {list(df_normalized.columns)}")
    return df_normalized

def _get_next_id_range(data_source, record_count):
    """è·å–ä¸‹ä¸€ä¸ªå¯ç”¨çš„IDèŒƒå›´"""
    id_ranges = {
        'national': (1000000000, 1999999999),
        'local': (2000000000, 2999999999),
        'coded': (3000000000, 3999999999),
        'external': (4000000000, 4999999999)
    }
    if data_source not in id_ranges:
        raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®æºç±»å‹: {data_source}")
    range_start, range_end = id_ranges[data_source]
    max_id_result = db.execute_query_safe(
        "SELECT ISNULL(MAX(id), ?) FROM è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶ WHERE id BETWEEN ? AND ?",
        (range_start - 1, range_start, range_end)
    )
    max_existing_id = max_id_result[0][0] if max_id_result and max_id_result[0] is not None else range_start - 1
    start_id = max_existing_id + 1
    end_id = start_id + record_count - 1
    if end_id > range_end:
        raise ValueError(f"{data_source}æ•°æ®IDèŒƒå›´å·²æ»¡ï¼Œæ— æ³•åˆ†é…{record_count}ä¸ªæ–°ID")
    logger.info(f"{data_source}æ•°æ®åˆ†é…IDèŒƒå›´: {start_id} - {end_id}")
    return start_id, end_id

def _generate_local_import_report(temp_count, inserted_count, matched_records,
                                unmatched_records, match_rate, unmatched_codes):
    """ç”Ÿæˆåœ°æ–¹ç‚¹æ•°æ®å¯¼å…¥æŠ¥å‘Š"""
    report = f"""åœ°æ–¹ç‚¹æ•°æ®å¯¼å…¥å®Œæˆï¼

ğŸ“Š å¯¼å…¥ç»Ÿè®¡ï¼š
â€¢ ä¸´æ—¶è¡¨å¯¼å…¥ï¼š{temp_count} æ¡è®°å½•
â€¢ ä¸»è¡¨æ’å…¥ï¼š{inserted_count} æ¡è®°å½•
â€¢ ç¼–ç åŒ¹é…æˆåŠŸï¼š{matched_records} æ¡è®°å½•
â€¢ ç¼–ç åŒ¹é…ç‡ï¼š{match_rate:.1f}%

âœ… å­—æ®µè‡ªåŠ¨ç”Ÿæˆï¼š
â€¢ typeï¼ˆæ”¶æ”¯ç±»åˆ«ï¼‰ï¼šå·²è‡ªåŠ¨å¡«å……
â€¢ type_nameï¼ˆå¸ç›®æŒ‡æ ‡åç§°ï¼‰ï¼šå·²è‡ªåŠ¨å¡«å……
â€¢ unit_nameï¼ˆå•ä½åç§°ï¼‰ï¼šå·²è‡ªåŠ¨å¡«å……
â€¢ ybmã€ybzæ ‡è®°ï¼šå·²æ­£ç¡®è®¾ç½®"""

    if unmatched_records > 0:
        report += f"""

âš ï¸  ç¼–ç åŒ¹é…é—®é¢˜ï¼š
â€¢ æœªåŒ¹é…è®°å½•ï¼š{unmatched_records} æ¡
â€¢ è¿™äº›è®°å½•çš„typeã€type_nameã€unit_nameå­—æ®µä¸ºç©ºå€¼"""

        if unmatched_codes:
            report += f"""
â€¢ æœªåŒ¹é…çš„ç¼–ç ï¼ˆå‰10ä¸ªï¼‰ï¼š
  {', '.join(unmatched_codes[:10])}"""
            if len(unmatched_codes) > 10:
                report += f"\n  ç­‰å…± {len(unmatched_codes)} ä¸ªç¼–ç "

        report += f"""

ğŸ’¡ å»ºè®®ï¼š
â€¢ æ£€æŸ¥è°ƒæŸ¥å“ç§ç¼–ç è¡¨æ˜¯å¦åŒ…å«è¿™äº›ç¼–ç 
â€¢ æˆ–è”ç³»ç®¡ç†å‘˜æ›´æ–°ç¼–ç è¡¨"""
    else:
        report += f"""

ğŸ‰ æ‰€æœ‰ç¼–ç åŒ¹é…æˆåŠŸï¼æ•°æ®è´¨é‡è‰¯å¥½ã€‚"""

    return report

@data_import_bp.route('/import_coded_data', methods=['POST'])
def import_coded_data():
    """å¯¼å…¥å·²ç¼–ç æ•°æ®"""
    @handle_errors
    def _import_coded_data():
        logger.info("å¼€å§‹å¯¼å…¥å·²ç¼–ç æ•°æ®")
        if 'file' not in request.files:
            return "æœªé€‰æ‹©æ–‡ä»¶", 400
        
        file = request.files['file']
        df, error, file_path = _process_uploaded_file(
            file, "å¯¼å…¥å·²ç¼–ç æ•°æ®", {'xlsx', 'xls'}, lambda p: excel_ops.read_excel(p)
        )

        if error:
            return error[0], error[1]
        
        try:
            required_columns = ['id', 'code']
            if not all(col in df.columns for col in required_columns):
                return f"Excelæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {[c for c in required_columns if c not in df.columns]}", 400
            
            db.ensure_performance_indexes()
            import_result = db.import_data(df, 'å·²ç»ç¼–ç å®Œæˆ')
            temp_count = import_result['successful_rows']
            
            with db.pool.get_cursor() as cursor:
                cursor.execute("CREATE NONCLUSTERED INDEX IX_temp_coded_id ON å·²ç»ç¼–ç å®Œæˆ (id)")
                
                update1_sql = '''UPDATE m SET m.code = t.code, m.ybm = '1'
                               FROM è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶ m JOIN å·²ç»ç¼–ç å®Œæˆ t ON t.id = m.id'''
                cursor.execute(update1_sql)
                update1_count = cursor.rowcount

                update2_sql = """UPDATE è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶ SET [note] = [type_name]+[note] COLLATE database_default, ybz='1' 
                               WHERE (code IS NOT NULL) AND (id IN (SELECT DISTINCT id FROM å·²ç»ç¼–ç å®Œæˆ)) AND ybz<>'1'"""
                cursor.execute(update2_sql)
                update2_count = cursor.rowcount

                update3_sql = '''UPDATE m SET type_name = c.å¸ç›®æŒ‡æ ‡åç§°, unit_name = c.å•ä½åç§°
                               FROM è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶ m JOIN è°ƒæŸ¥å“ç§ç¼–ç  c ON m.code = c.å¸ç›®ç¼–ç 
                               WHERE m.code IS NOT NULL AND ybz='1' '''
                cursor.execute(update3_sql)
                update3_count = cursor.rowcount
            
            db.optimize_table_statistics('è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶')
            
            return (f"å·²ç¼–ç æ•°æ®å¯¼å…¥æˆåŠŸï¼\n"
                    f"â€¢ å¯¼å…¥åˆ°ä¸´æ—¶è¡¨ï¼š{temp_count} æ¡\n"
                    f"â€¢ æ›´æ–°ç¼–ç ä¿¡æ¯ï¼š{update1_count} æ¡\n"
                    f"â€¢ æ›´æ–°å¤‡æ³¨ä¿¡æ¯ï¼š{update2_count} æ¡\n"
                    f"â€¢ æ›´æ–°ç±»å‹åç§°ï¼š{update3_count} æ¡")
        finally:
            _cleanup_file(file_path)
    return _import_coded_data()

@data_import_bp.route('/import_local_data', methods=['POST'])
def import_local_data():
    """å¯¼å…¥åœ°æ–¹ç‚¹æ•°æ®"""
    @handle_errors
    def _import_local_data():
        logger.info("å¼€å§‹å¯¼å…¥åœ°æ–¹ç‚¹æ•°æ®")
        if 'file' not in request.files:
            return "æœªé€‰æ‹©æ–‡ä»¶", 400
        
        file = request.files['file']
        df, error, file_path = _process_uploaded_file(
            file, "å¯¼å…¥åœ°æ–¹ç‚¹æ•°æ®", {'xlsx', 'xls'}, lambda p: excel_ops.read_excel(p)
        )

        if error:
            return error[0], error[1]
        
        try:
            # æ ‡å‡†åŒ–Excelåˆ—å
            df_normalized = _normalize_local_data_columns(df)

            # æ•°æ®éªŒè¯å’Œæ¸…ç†
            logger.info("å¼€å§‹éªŒè¯å’Œæ¸…ç†åœ°æ–¹ç‚¹æ•°æ®")

            # éªŒè¯æˆ·ä»£ç æ ¼å¼ï¼ˆæ¸…ç†.0åç¼€ï¼‰
            if 'hudaima' in df_normalized.columns:
                original_hudaima_sample = df_normalized['hudaima'].head(3).tolist()
                logger.info(f"åŸå§‹æˆ·ä»£ç æ ·ä¾‹: {original_hudaima_sample}")

                # æ¸…ç†æˆ·ä»£ç ä¸­çš„.0åç¼€
                df_normalized['hudaima'] = df_normalized['hudaima'].astype(str).str.replace(r'\.0$', '', regex=True)
                cleaned_hudaima_sample = df_normalized['hudaima'].head(3).tolist()
                logger.info(f"æ¸…ç†åæˆ·ä»£ç æ ·ä¾‹: {cleaned_hudaima_sample}")

            # éªŒè¯riåˆ—æ•°æ®
            if 'ri' in df_normalized.columns:
                ri_sample = df_normalized['ri'].head(5).tolist()
                logger.info(f"riåˆ—æ•°æ®æ ·ä¾‹: {ri_sample}")

                # ç»Ÿè®¡riåˆ—çš„æ•°æ®åˆ†å¸ƒ
                ri_valid_count = df_normalized['ri'].apply(lambda x: str(x) not in ['nan', '', 'None'] and str(x).isdigit()).sum()
                ri_total_count = len(df_normalized)
                logger.info(f"riåˆ—æœ‰æ•ˆæ•°æ®: {ri_valid_count}/{ri_total_count} æ¡")

            # éªŒè¯ç¼–ç æœ‰æ•ˆæ€§
            if 'bianma' in df_normalized.columns:
                # æ£€æŸ¥ç¼–ç æ ¼å¼
                valid_code_pattern = df_normalized['bianma'].str.match(r'^\d{6}$', na=False)
                valid_codes_count = valid_code_pattern.sum()
                total_codes_count = len(df_normalized)
                logger.info(f"ç¼–ç æ ¼å¼éªŒè¯: {valid_codes_count}/{total_codes_count} æ¡ç¬¦åˆ6ä½æ•°å­—æ ¼å¼")

                # è·å–ç¼–ç æ ·ä¾‹
                code_sample = df_normalized['bianma'].head(5).tolist()
                logger.info(f"ç¼–ç æ ·ä¾‹: {code_sample}")

                # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ç¼–ç ï¼ˆåŒä¸€æˆ·åŒä¸€å¹´æœˆï¼‰
                duplicate_check = df_normalized.groupby(['hudaima', 'nian', 'yue', 'bianma']).size()
                duplicates = duplicate_check[duplicate_check > 1]
                if len(duplicates) > 0:
                    logger.warning(f"å‘ç°é‡å¤è®°å½•: {len(duplicates)} ç»„é‡å¤çš„æˆ·ä»£ç -å¹´æœˆ-ç¼–ç ç»„åˆ")
                else:
                    logger.info("æœªå‘ç°é‡å¤è®°å½•")

            # å¯¼å…¥æ•°æ®åˆ°åœ°æ–¹ç‚¹å¾…å¯¼å…¥è¡¨
            import_result = db.import_data(df_normalized, 'åœ°æ–¹ç‚¹å¾…å¯¼å…¥')
            temp_count = import_result['successful_rows']
            logger.info(f"åœ°æ–¹ç‚¹æ•°æ®æˆåŠŸå…¥åº“åˆ°ä¸´æ—¶è¡¨ï¼Œå…± {temp_count} æ¡è®°å½•")

            if temp_count == 0:
                return "æ²¡æœ‰æ–°çš„åœ°æ–¹ç‚¹æ•°æ®éœ€è¦å¯¼å…¥ã€‚", 200

            # è·å–åœ°æ–¹ç‚¹æ•°æ®çš„IDåˆ†é…èŒƒå›´
            local_id_start, local_id_end = _get_next_id_range('local', temp_count)
            logger.info(f"åœ°æ–¹ç‚¹æ•°æ®åˆ†é…IDèŒƒå›´: {local_id_start} - {local_id_end}")

            # æ’å…¥æ•°æ®åˆ°è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶è¡¨ï¼ˆæ”¯æŒåŠ¨æ€æ—¥æœŸå¤„ç†å’Œå­—æ®µè‡ªåŠ¨ç”Ÿæˆï¼‰
            logger.info("å¼€å§‹æ’å…¥åœ°æ–¹ç‚¹æ•°æ®åˆ°ä¸»è¡¨ï¼Œæ”¯æŒä»Excelä¸­çš„å¹´æœˆæ—¥å­—æ®µåŠ¨æ€æ„å»ºæ—¥æœŸï¼Œå¹¶è‡ªåŠ¨ç”Ÿæˆtypeã€type_nameã€unit_nameå­—æ®µ")
            insert_sql = f'''INSERT INTO [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶] (
    [hudm],
    [code],
    [amount],
    [money],
    [year],
    [month],
    [z_guid],
    [date],
    [id],
    [type],
    [type_name],
    [unit_name],
    [ybm],
    [ybz],
    [wton],
    [ntow]
)
SELECT
    cast(d.[hudaima] as varchar(50)) as hudm,
    cast(d.[bianma] as varchar(50)) as code,
    coalesce(try_cast(d.[shuliang] as real), 0) as amount,
    coalesce(try_cast(d.[jine] as real), 0) as money,
    cast(d.[nian] as varchar(4)) as year,
    CASE
        WHEN LEN(cast(d.[yue] as varchar(2))) = 1 THEN '0' + cast(d.[yue] as varchar(2))
        ELSE cast(d.[yue] as varchar(2))
    END as month,
    NEWID() as z_guid,
    CASE
        WHEN d.[ri] IS NOT NULL AND d.[ri] != 'nan' AND d.[ri] != '' AND d.[ri] != 'None'
             AND TRY_CAST(d.[ri] AS INT) IS NOT NULL
             AND TRY_CAST(d.[nian] AS INT) IS NOT NULL
             AND TRY_CAST(d.[yue] AS INT) IS NOT NULL
             AND TRY_CAST(d.[ri] AS INT) BETWEEN 1 AND 31
             AND TRY_CAST(d.[yue] AS INT) BETWEEN 1 AND 12
             AND TRY_CAST(d.[nian] AS INT) BETWEEN 1900 AND 2100
        THEN TRY_CAST(
            cast(d.[nian] as varchar(4)) + '-' +
            RIGHT('0' + cast(d.[yue] as varchar(2)), 2) + '-' +
            RIGHT('0' + cast(d.[ri] as varchar(2)), 2)
            AS SMALLDATETIME
        )
        ELSE TRY_CAST(
            cast(d.[nian] as varchar(4)) + '-' +
            RIGHT('0' + cast(d.[yue] as varchar(2)), 2) + '-01'
            AS SMALLDATETIME
        )
    END as date,
    {local_id_start} + ROW_NUMBER() OVER (ORDER BY d.[hudaima], d.[nian], d.[yue]) - 1 as id,
    CAST(ISNULL(c.æ”¶æ”¯ç±»åˆ«, 0) AS INT) as type,
    CAST(ISNULL(c.å¸ç›®æŒ‡æ ‡åç§°, '') AS VARCHAR(255)) as type_name,
    CAST(ISNULL(c.å•ä½åç§°, '') AS VARCHAR(255)) as unit_name,
    CAST('1' AS VARCHAR(1)) as ybm,
    CAST('1' AS VARCHAR(1)) as ybz,
    CAST('1' AS VARCHAR(1)) as wton,
    CAST('0' AS VARCHAR(1)) as ntow
FROM [åœ°æ–¹ç‚¹å¾…å¯¼å…¥] d
LEFT JOIN [è°ƒæŸ¥å“ç§ç¼–ç ] c ON d.[bianma] = c.å¸ç›®ç¼–ç 
WHERE NOT EXISTS (
    SELECT 1 FROM [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶] existing
    WHERE existing.hudm = cast(d.[hudaima] as varchar(50))
        AND existing.year = cast(d.[nian] as varchar(4))
        AND existing.month = CASE
            WHEN LEN(cast(d.[yue] as varchar(2))) = 1 THEN '0' + cast(d.[yue] as varchar(2))
            ELSE cast(d.[yue] as varchar(2))
        END
        AND existing.code = cast(d.[bianma] as varchar(50))
        AND existing.id BETWEEN 2000000000 AND 2999999999
)'''

            # ä½¿ç”¨äº‹åŠ¡æ–¹å¼æ‰§è¡Œæ‰€æœ‰SQLæ“ä½œ
            with db.pool.get_cursor() as cursor:
                cursor.execute(insert_sql)
                inserted_count = cursor.rowcount
                logger.info(f"åœ°æ–¹ç‚¹æ•°æ®æˆåŠŸåˆå¹¶åˆ°ä¸»è¡¨ï¼Œå…±æ’å…¥ {inserted_count} æ¡è®°å½•")

                # ç»Ÿè®¡ç¼–ç åŒ¹é…æƒ…å†µ
                if inserted_count > 0:
                    match_stats_sql = f'''
                    SELECT
                        COUNT(*) as total_records,
                        COUNT(CASE WHEN type_name IS NOT NULL AND type_name != '' THEN 1 END) as matched_records,
                        COUNT(CASE WHEN type_name IS NULL OR type_name = '' THEN 1 END) as unmatched_records
                    FROM [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶]
                    WHERE id BETWEEN {local_id_start} AND {local_id_start + inserted_count - 1}
                    '''
                    cursor.execute(match_stats_sql)
                    stats_result = cursor.fetchone()

                    if stats_result:
                        total_records, matched_records, unmatched_records = stats_result
                        match_rate = (matched_records / total_records * 100) if total_records > 0 else 0
                        logger.info(f"ç¼–ç åŒ¹é…ç»Ÿè®¡: æ€»è®°å½• {total_records} æ¡, åŒ¹é…æˆåŠŸ {matched_records} æ¡, "
                                  f"æœªåŒ¹é… {unmatched_records} æ¡, åŒ¹é…ç‡ {match_rate:.1f}%")
                    else:
                        matched_records = 0
                        unmatched_records = inserted_count
                        match_rate = 0
                        logger.warning("æ— æ³•è·å–ç¼–ç åŒ¹é…ç»Ÿè®¡ä¿¡æ¯")
                else:
                    matched_records = 0
                    unmatched_records = 0
                    match_rate = 0

                # è·å–æœªåŒ¹é…ç¼–ç çš„è¯¦ç»†ä¿¡æ¯ï¼ˆç”¨äºæŠ¥å‘Šï¼‰
                unmatched_codes = []
                if unmatched_records > 0:
                    unmatched_codes_sql = f'''
                    SELECT DISTINCT code
                    FROM [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶]
                    WHERE id BETWEEN {local_id_start} AND {local_id_start + inserted_count - 1}
                        AND (type_name IS NULL OR type_name = '')
                        AND code IS NOT NULL
                    ORDER BY code
                    '''
                    cursor.execute(unmatched_codes_sql)
                    unmatched_codes = [row[0] for row in cursor.fetchall()]

                    if unmatched_codes:
                        logger.warning(f"æœªåŒ¹é…çš„ç¼–ç : {', '.join(unmatched_codes[:10])}" +
                                     (f" ç­‰å…± {len(unmatched_codes)} ä¸ª" if len(unmatched_codes) > 10 else ""))

            # ç”Ÿæˆè¯¦ç»†çš„å¯¼å…¥æŠ¥å‘Š
            return _generate_local_import_report(temp_count, inserted_count, matched_records,
                                               unmatched_records, match_rate, unmatched_codes)
        finally:
            _cleanup_file(file_path)
    return _import_local_data()

@data_import_bp.route('/import_national_data', methods=['POST'])
def import_national_data():
    """å¯¼å…¥å›½å®¶ç‚¹æ•°æ® (CSVæ ¼å¼)"""
    @handle_errors
    def _import_national_data():
        logger.info("å¼€å§‹å¯¼å…¥å›½å®¶ç‚¹æ•°æ® (CSVæ ¼å¼)")
        if 'file' not in request.files:
            return "æœªé€‰æ‹©æ–‡ä»¶", 400

        file = request.files['file']
        df, error, file_path = _process_uploaded_file(
            file, "å¯¼å…¥å›½å®¶ç‚¹æ•°æ®", {'csv'}, _read_and_process_csv
        )

        if error:
            return error[0], error[1]
        
        try:
            required_columns = ['SID', 'ç¼–ç ', 'å“å', 'äººç ', 'åˆ›å»ºæ—¶é—´']
            if not all(col in df.columns for col in required_columns):
                return f"CSVæ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—: {[c for c in required_columns if c not in df.columns]}", 400

            import_result = db.import_data(df, 'å›½å®¶ç‚¹å¾…å¯¼å…¥')
            temp_count = import_result['successful_rows']
            logger.info(f"å›½å®¶ç‚¹æ•°æ®æˆåŠŸå…¥åº“åˆ°ä¸´æ—¶è¡¨ï¼Œå…± {temp_count} æ¡è®°å½•")

            if temp_count == 0:
                return "æ²¡æœ‰æ–°çš„å›½å®¶ç‚¹æ•°æ®éœ€è¦å¯¼å…¥ã€‚", 200

            # æ•°æ®éªŒè¯ï¼šæ£€æŸ¥æœ‰æ•ˆè®°å½•æ•°
            logger.info("å¼€å§‹éªŒè¯å›½å®¶ç‚¹æ•°æ®")
            valid_record_result = db.execute_query_safe("""
                SELECT COUNT(*) FROM å›½å®¶ç‚¹å¾…å¯¼å…¥
                WHERE ([SID] IS NOT NULL AND [SID] <> '')
                    AND (TRY_CONVERT(DATETIME, [åˆ›å»ºæ—¶é—´], 120) IS NOT NULL)
                    AND ([ç¼–ç ] IS NOT NULL AND [ç¼–ç ] <> '')
                    AND ([å“å] IS NOT NULL AND [å“å] <> '')
                    AND (([äººç ] IS NOT NULL AND [äººç ] <> '') OR ([äººä»£ç ] IS NOT NULL AND [äººä»£ç ] <> ''))
            """)
            valid_record_count = valid_record_result[0][0] if valid_record_result else 0
            logger.info(f"æœ‰æ•ˆè®°å½•æ•°: {valid_record_count}")

            inserted_count = 0
            updated_count = 0
            type_updated_count = 0

            if valid_record_count > 0:
                # è·å–å›½å®¶ç‚¹æ•°æ®çš„IDåˆ†é…èŒƒå›´
                national_id_start, _ = _get_next_id_range('national', valid_record_count)
                logger.info(f"å›½å®¶ç‚¹æ•°æ®åˆ†é…IDèŒƒå›´èµ·å§‹: {national_id_start}")

                # æ’å…¥æ•°æ®åˆ°è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶è¡¨
                logger.info("å¼€å§‹æ’å…¥å›½å®¶ç‚¹æ•°æ®åˆ°ä¸»è¡¨")
                insert_sql = f'''
                INSERT INTO [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶] (
                    hudm, code, amount, money, note, person, year, month, z_guid, date,
                    type, id, type_name, unit_name, ybm, ybz, wton, ntow
                )
                SELECT
                    LEFT([SID], 12) + LEFT(RIGHT([SID], 5), 3) AS hudm,
                    CAST([ç¼–ç ] AS VARCHAR(50)) AS code,
                    [æ•°é‡] AS amount,
                    [é‡‘é¢] AS money,
                    CAST(ISNULL([è®°è´¦è¯´æ˜], '') AS VARCHAR(255)) AS note,
                    CAST(ISNULL([äººç ], ISNULL([äººä»£ç ], '')) AS VARCHAR(255)) AS person,
                    CAST(ISNULL([å¹´], YEAR(TRY_CONVERT(DATETIME, [åˆ›å»ºæ—¶é—´], 120))) AS VARCHAR(4)) AS year,
                    RIGHT('0' + CAST(ISNULL([æœˆ], MONTH(TRY_CONVERT(DATETIME, [åˆ›å»ºæ—¶é—´], 120))) AS VARCHAR(2)), 2) AS month,
                    NEWID() AS z_guid,
                    TRY_CONVERT(SMALLDATETIME, [åˆ›å»ºæ—¶é—´], 120) AS date,
                    0 AS type,
                    {national_id_start} + ROW_NUMBER() OVER (ORDER BY [SID]) - 1 AS id,
                    CAST([å“å] AS VARCHAR(255)) AS type_name,
                    CAST('' AS VARCHAR(255)) AS unit_name,
                    CAST('' AS VARCHAR(1)) AS ybm,
                    CAST('1' AS VARCHAR(1)) AS ybz,
                    CAST('1' AS VARCHAR(1)) AS wton,
                    CAST('0' AS VARCHAR(1)) AS ntow
                FROM å›½å®¶ç‚¹å¾…å¯¼å…¥
                WHERE
                    ([SID] IS NOT NULL AND [SID] <> '') AND
                    (TRY_CONVERT(DATETIME, [åˆ›å»ºæ—¶é—´], 120) IS NOT NULL) AND
                    ([ç¼–ç ] IS NOT NULL AND [ç¼–ç ] <> '') AND
                    ([å“å] IS NOT NULL AND [å“å] <> '') AND
                    (([äººç ] IS NOT NULL AND [äººç ] <> '') OR ([äººä»£ç ] IS NOT NULL AND [äººä»£ç ] <> ''))
                '''

                # ä½¿ç”¨äº‹åŠ¡æ–¹å¼æ‰§è¡Œæ‰€æœ‰SQLæ“ä½œ
                with db.pool.get_cursor() as cursor:
                    cursor.execute(insert_sql)
                    inserted_count = cursor.rowcount
                    logger.info(f"å›½å®¶ç‚¹æ•°æ®æˆåŠŸåˆå¹¶åˆ°ä¸»è¡¨ï¼Œå…±æ’å…¥ {inserted_count} æ¡è®°å½•")

                    # æ›´æ–°ç¼–ç åŒ¹é…ä¿¡æ¯
                    if inserted_count > 0:
                        update_sql = f'''UPDATE [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶]
                            SET type_name = c.å¸ç›®æŒ‡æ ‡åç§°, unit_name = c.å•ä½åç§°
                            FROM [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶] t INNER JOIN [è°ƒæŸ¥å“ç§ç¼–ç ] c ON t.code = c.å¸ç›®ç¼–ç 
                            WHERE t.code IS NOT NULL AND t.ybz='1' AND t.id >= {national_id_start}'''
                        cursor.execute(update_sql)
                        updated_count = cursor.rowcount
                        logger.info(f"å›½å®¶ç‚¹æ•°æ®ç¼–ç åŒ¹é…å®Œæˆï¼Œå…±æ›´æ–° {updated_count} æ¡è®°å½•")

                        # æ›´æ–°æ”¶æ”¯ç±»åˆ«
                        type_update_sql = f'''UPDATE [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶]
                            SET type = CAST(c.æ”¶æ”¯ç±»åˆ« AS INT)
                            FROM [è°ƒæŸ¥ç‚¹å°è´¦åˆå¹¶] t INNER JOIN [è°ƒæŸ¥å“ç§ç¼–ç ] c ON t.code = c.å¸ç›®ç¼–ç 
                            WHERE t.id >= {national_id_start} AND t.code IS NOT NULL AND c.æ”¶æ”¯ç±»åˆ« IS NOT NULL'''
                        cursor.execute(type_update_sql)
                        type_updated_count = cursor.rowcount
                        logger.info(f"æ”¶æ”¯ç±»åˆ«è‡ªåŠ¨å¡«å……å®Œæˆï¼Œå…±æ›´æ–° {type_updated_count} æ¡è®°å½•çš„typeå­—æ®µ")

            # æ„å»ºè¿”å›æ¶ˆæ¯
            summary_message = f"å›½å®¶ç‚¹æ•°æ®å¯¼å…¥å®Œæˆï¼\n"
            summary_message += f"â€¢ å¯¼å…¥åˆ°ä¸´æ—¶è¡¨ï¼š{temp_count} æ¡\n"
            summary_message += f"â€¢ æ’å…¥åˆ°ä¸»è¡¨ï¼š{inserted_count} æ¡\n"
            summary_message += f"â€¢ ç¼–ç åŒ¹é…æ›´æ–°ï¼š{updated_count} æ¡\n"
            summary_message += f"â€¢ æ”¶æ”¯ç±»åˆ«å¡«å……ï¼š{type_updated_count} æ¡\n"

            invalid_count = temp_count - valid_record_count
            if invalid_count > 0:
                summary_message += f"â€¢ æ— æ•ˆè®°å½•ï¼ˆæœªå¯¼å…¥ï¼‰ï¼š{invalid_count} æ¡"

            return summary_message
        finally:
            _cleanup_file(file_path)
    return _import_national_data()